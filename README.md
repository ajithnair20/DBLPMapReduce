# DBLP Map Reduce

This project aims at creating a map/reduce program for parallel processing of the publically available [DBLP dataset](https://dblp.uni-trier.de/xml/) that contains entries for various publications at many different venues (e.g., conferences and journals). We perform multiple map/reduce operations on the dataset to identify differernt statistics pertaining to the authors of the publications.
The type of map/reduce operations performed are :
  - Calculating authorship score of authors.
  - Finding Mean, Median and Maximum of the number of co-authors across different articles and publications for an individual authors.
  - Finding the number of authors in papers and assigning them to different bins based on range of authors.
  - Finding the number of unique collaborations of every authors in the DBLP dataset.

# Setting up the environment
#### Compiling code and generating jar file
Open a terminal and clone the repository by using the following command:
```sh
$ git clone https://ajithnair08@bitbucket.org/ajithnair08/ajithjayaraman_nair_hw2.git
```
Traverse to the root directory of the project and compile the code and create jar of the application using the below commands:
```sh
$ sbt clean compile assembly
```
On running these commands, the test cases are run and a new "Target" folder is created which inclues the jar file. The results of the test cases are displayed in the terminal.
#### Setting up hadoop
The map/reduce operations are performed on the Apache Hadoop platform. In order to perform the operations you need to setup hadoop on your system. Alternatively, you can use a virtual machine (VM) of [Hortonworks Sandbox](https://www.cloudera.com/downloads/hortonworks-sandbox.html), a preconfigured Apache Hadoop installation with a comprehensive software stack. To run the VM, you can install vmWare or VirtualBox. Instructions to setup hadoop on a Linux based system can found at [Installing Hadoop Guide.](https://www.ashessin.com/BD_STTP_2016/)

After setting up hadoop execute the below mentioned commands:

 Start HDFS and Hadoop YARN
```sh
$ start-dfs.sh
$ start-yarn.sh
```
Create a directory in HDFS to store the input file for the program
```sh
$ hdfs dfs -mkdir /input
```

Traverse to the directory in which the input file i.e the DBLP XML dataset is stored and perform the following commands
```sh
$ hdfs dfs -put dblp.xml /input
```

# Running the Application
Traverse to the folder in which the jar file has been generated and perform the following command.
```sh
$ hadoop jar AjithJayaraman_Nair_HW2-assembly-1.0 /input /output
```
The output directory is generated by hadoop. To execute the program again delete the output folder as hadoop raises error when the output folder already exists. Alternatively, you can specify a different output file directory. To delete the output directory, execute the following commands.
```sh
$ hdfs dfs -rm -r /output
```
# Map Reduce Operations
#### Authorship Score
Authorship score is calculated using the following formula. The total score for a paper is one. Hence, if an author published 15 papers as the single author without any co-authors, then her score will be 15. For a paper with multiple co-authors the score will be computed using a split weight as the following. First, each co-author receives 1/N score where N is the number of co-authors. Then, the score of the last co-author is credited 1/(4N) leaving it 3N/4 of the original score. The next co-author to the left is debited 1/(4N) and the process repeats until the first author is reached. For example, for a single author the score is one. For two authors, the original score is 0.5. Then, for the last author the score is updated 0.53/4 = 0.5-0.125 = 0.375 and the first author's score is 0.5+0.125 = 0.625. The process repeats the same way for N co-authors.
The progarms calucates the cumulative authorship score of every author in the DBLP dataset. The output of the reducer is in the format:
```sh
<author_name>,<authorship_score>
```
Output is generated at the path: _output/authorshipscore_
##### Output Excerpt:
```sh
albert maier,0.625
alejandro p. buchmann,0.6875
anja theuner,1.0
benjamin hurwitz,0.375
bernd walter,0.94921875
bernhard beckert,1.0
birgit wendholt,1.0
birgit wesche,1.0
brigitte bartsch-spörl,2.0
burkhard kehrbusch,0.375
```

The top 100 highest and lowest authorship score can be viewed at path /Visualisation Chart and Outputs/
#### Mean, Median and Maximum of the number of co-authors
The program calculates the mean, median and maximum count of the co-authors of an author across the DBLP dataset i.e. if an author has worked with 4 co-authors in a paper and with 6 co-authors in another and 10 co-authors in a third paper. Then the max value for the author is 8, median value is 6 and mean value is 6.67. The output of the reducer is in the format:
```sh
<author_name>,<mean,median,max>
```
Output is generated at the path: _output/meanmedianmax_
##### Output  Excerpt:
```sh
daniel genkin,9.0,9.0,9
daniel gruss,9.0,9.0,9
david beech,0.5,0.5,1
dimitrios georgakopoulos,2.0,2.0,2
egon börger,0.0,0.0,0
erich gehlen,2.25,2.0,4
farshad nayeri,1.5,1.5,2
frank manola,0.375,0.0,2
```
#### Bins based on range of authors in a publication/article
Programs generates bins of the values (1 Author), (2-3 Authors), (4-10 Authors), (11-30 Authors), (31-300 Authors), (More than 300 Authors) and assign papers to them based on the number of authors. The output of the reducer is in the format:
```sh
<Range_name>,<value>
```
Output is generated at the path: _output/coauthorrange_
##### Output Excerpt:
```sh
(1 Author),3145455
(2-3 Authors),2511427
(4-10 Authors),1439466
(11-30 Authors),22324
(31-300 Authors),566
```
Refer to the Chart at path /Visualisation Chart and Outputs/Author Range.jpg
#### Unique collaborations of every authors in the DBLP dataset
Program outputs the number of authors with whom an author has collaborated with.
The output is in the format:
```sh
<auhtor_name>,<co-author_name>
```
Output is generated at the path: _output/authorcollaboration_
##### Output Excerpt:
```sh
albert maier,4
alejandro p. buchmann,4
benjamin hurwitz,1
bernd walter,5
burkhard kehrbusch,1
cetin ozbutun,1
christoph beierle,3
claus-rainer rollinger,1
daniel genkin,9
daniel gruss,9
david beech,1
```
# Implementation on AWS EMR
The program has been implemented on AWS EMR as well. You can refer to the [link](https://www.youtube.com/watch?v=tdtSRPymgCI&feature=youtu.be) for steps to setup Map Reduce using Amazon EMR.
The video includes details of deployment of the jar file on AWS S3 and setting up EMR.  